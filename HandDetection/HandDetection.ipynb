{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Import Necessary Libraries:"
      ],
      "metadata": {
        "id": "ZPe1L8jjKbI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2  # Assuming OpenCV is allowed\n",
        "import numpy as np\n",
        "# Import other libraries for Fuzzy Logic or Neural Networks (if applicable)\n"
      ],
      "metadata": {
        "id": "2uCg7BZrK13l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Functions for Image Processing (Individual Task 1):"
      ],
      "metadata": {
        "id": "LHp4uG1lKgK0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finger Detection and Counting:"
      ],
      "metadata": {
        "id": "VmASxdfGKiLo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def detect_fingers(image):\n",
        "  \"\"\"\n",
        "  Detects the number of fingers in a grayscale image using blob analysis.\n",
        "\n",
        "  Args:\n",
        "      image: A grayscale image of the hand.\n",
        "\n",
        "  Returns:\n",
        "      The number of fingers detected in the image.\n",
        "  \"\"\"\n",
        "\n",
        "  # Apply thresholding to convert the grayscale image to binary\n",
        "  thresh = cv2.threshold(image, 127, 255, cv2.THRESH_BINARY)[1]\n",
        "\n",
        "  # Find contours in the thresholded image\n",
        "  contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "  # Filter contours based on area and convexity\n",
        "  filtered_contours = []\n",
        "  for cnt in contours:\n",
        "    area = cv2.contourArea(cnt)\n",
        "    hull = cv2.convexHull(cnt)\n",
        "    hull_area = cv2.contourArea(hull)\n",
        "    if 100 < area < 1000 and hull_area > 0.8 * area:  # Adjust thresholds as needed\n",
        "      filtered_contours.append(cnt)\n",
        "\n",
        "  # Count the number of filtered contours (assumed to be fingers)\n",
        "  finger_count = len(filtered_contours)\n",
        "\n",
        "  return finger_count\n",
        "\n",
        "# Example usage (assuming you have a grayscale image named 'hand.jpg')\n",
        "image = cv2.imread('hand.jpg', 0)  # Read the image in grayscale\n",
        "finger_count = detect_fingers(image)\n",
        "print(f\"Number of fingers detected: {finger_count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAekNH_CK4Lz",
        "outputId": "5fcb8a40-5250-4278-fb68-d6fd82befcec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of fingers detected: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hand Motion Detection:"
      ],
      "metadata": {
        "id": "1_E-Z0jlKkNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def detect_hand_motion(image, prev_frame):\n",
        "  \"\"\"\n",
        "  Detects the direction of hand motion using Farneback optical flow.\n",
        "\n",
        "  Args:\n",
        "      image: The current image frame.\n",
        "      prev_frame: The previous image frame.\n",
        "\n",
        "  Returns:\n",
        "      A string representing the direction of motion: \"left\", \"right\", \"up\", \"down\", or \"none\".\n",
        "  \"\"\"\n",
        "\n",
        "  # Convert images to grayscale for optical flow calculation\n",
        "  gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "  gray_prev_frame = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "  # Calculate dense optical flow using Farneback method\n",
        "  # You can adjust parameters like `5`, `15`, `3`, `5.0`, `1.2` for better results\n",
        "  # based on your video characteristics.\n",
        "  farneback_params = dict(fx=5, fy=15, fxwin=3, minDisparity=0,\n",
        "                         sadWindowRadius=5, disparityRange=1.2, mode=cv2.OPTFLOW_FARNEBACK)\n",
        "  flow = cv2.calcOpticalFlowFarneback(gray_prev_frame, gray_image, None, **farneback_params)\n",
        "\n",
        "  # Calculate average horizontal and vertical motion\n",
        "  avg_hor_motion = np.mean(flow[..., 0])\n",
        "  avg_ver_motion = np.mean(flow[..., 1])\n",
        "\n",
        "  # Determine motion direction based on thresholds (adjust as needed)\n",
        "  motion_direction = None\n",
        "  threshold = 5  # Adjust threshold for sensitivity\n",
        "  if abs(avg_hor_motion) > threshold:\n",
        "    motion_direction = \"left\" if avg_hor_motion < -threshold else \"right\"\n",
        "  elif abs(avg_ver_motion) > threshold:\n",
        "    motion_direction = \"up\" if avg_ver_motion < -threshold else \"down\"\n",
        "  else:\n",
        "    motion_direction = \"none\"\n",
        "\n",
        "  return motion_direction\n",
        "\n",
        "# Example usage (assuming you have image and prev_frame)\n",
        "motion_direction = detect_hand_motion(image, prev_frame)\n",
        "print(f\"Hand motion direction: {motion_direction}\")\n"
      ],
      "metadata": {
        "id": "EK1xgHklK598"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hand Gesture Recognition:"
      ],
      "metadata": {
        "id": "h_uXImtjKmN8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "def extract_features(image):\n",
        "  # Implement feature extraction methods here (Hu Moments, Convexity Defects, etc.)\n",
        "  # ...\n",
        "  features = np.array([feature1, feature2, ...])\n",
        "  return features\n",
        "\n",
        "def train_classifiers(training_images, training_labels):\n",
        "  # Extract features from training images\n",
        "  features = []\n",
        "  for image in training_images:\n",
        "    features.append(extract_features(image))\n",
        "  features = np.array(features)\n",
        "\n",
        "  # Train SVM and Decision Tree classifiers\n",
        "  svm_clf = SVC()\n",
        "  svm_clf.fit(features, training_labels)\n",
        "\n",
        "  tree_clf = DecisionTreeClassifier()\n",
        "  tree_clf.fit(features, training_labels)\n",
        "\n",
        "  return svm_clf, tree_clf\n",
        "\n",
        "def recognize_gesture(image, svm_clf, tree_clf):\n",
        "  # Extract features from the image\n",
        "  features = extract_features(image)\n",
        "\n",
        "  # Predict using both SVM and Decision Tree\n",
        "  svm_prediction = svm_clf.predict(features.reshape(1, -1))[0]\n",
        "  tree_prediction = tree_clf.predict(features.reshape(1, -1))[0]\n",
        "\n",
        "  # Combine predictions (e.g., majority vote or weighted average)\n",
        "  combined_prediction = svm_prediction  # Replace with your combination strategy\n",
        "  return combined_prediction\n",
        "\n",
        "# Example usage (assuming you have training data)\n",
        "training_images, training_labels = ...  # Load your training data\n",
        "svm_clf, tree_clf = train_classifiers(training_images, training_labels)\n",
        "\n",
        "image = ...  # Load the image for gesture recognition\n",
        "gesture_type = recognize_gesture(image, svm_clf, tree_clf)\n",
        "print(f\"Predicted gesture: {gesture_type}\")\n"
      ],
      "metadata": {
        "id": "yTXFjlieK7fL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hand Orientation Detection:"
      ],
      "metadata": {
        "id": "vNcfg8xTKnwc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_hand_orientation(image):\n",
        "    # Implement orientation detection using convexity defects or deep learning\n",
        "    # (replace with your chosen method)\n",
        "    # ...\n",
        "    return orientation  # e.g., \"left\", \"right\", \"palm\", \"back\"\n"
      ],
      "metadata": {
        "id": "3KfekDLnK-bT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Develop AI-Based Identification (Individual Task 2, Group Task):"
      ],
      "metadata": {
        "id": "8ueAn7ocKp7N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fuzzy Logic:"
      ],
      "metadata": {
        "id": "hcVFQlA8KsG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define fuzzy sets and rules to represent hand features and gestures\n",
        "# ...\n",
        "def identify_gesture_fuzzy(image):\n",
        "    # Apply fuzzy logic rules to classify gestures\n",
        "\n",
        "    return gesture_type\n"
      ],
      "metadata": {
        "id": "r_c8BjqwLAO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural Networks (if allowed):"
      ],
      "metadata": {
        "id": "QVe1hCeiKuDO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def identify_gesture_cnn(image):\n",
        "\n",
        "    return gesture_type\n"
      ],
      "metadata": {
        "id": "FqJHQW9FLCWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ntegrate Code and GUI (Group Task):"
      ],
      "metadata": {
        "id": "nKY7554NKxGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tkinter as tk\n",
        "\n",
        "def update_display(image, gesture_text):\n",
        "\n",
        "\n",
        "\n",
        "def video_loop():\n",
        "    cap = cv2.VideoCapture(0)  # Capture video from webcam\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Process the frame using image processing and AI functions\n",
        "        finger_count = detect_fingers(frame)\n",
        "        motion_direction = detect_hand_motion(frame, prev_frame)\n",
        "        gesture_type = identify_gesture(frame)  # Choose fuzzy or CNN approach\n",
        "        orientation = detect_hand_orientation(frame)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tCdo8uYXLEJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "   # Update the display with processed information"
      ],
      "metadata": {
        "id": "_-cnpcbvo76u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "        update_display(frame, f\"Gesture: {gesture_type}\\n\"\n",
        "                               f\"Fingers: {finger_count}\\n\"\n",
        "                               f\"Motion: {motion_direction}\\n\"\n",
        "                               f\"Orientation: {orientation}\")\n",
        "\n",
        "        prev_frame = frame.copy()\n",
        "        cv2.waitKey(1)\n",
        "\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "KLyA2y3vo6pm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create the GUI window and buttons (replace with your desired layout)"
      ],
      "metadata": {
        "id": "xiJAyLbao3ze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "root = tk.Tk()\n",
        "root.title(\"Hand Gesture Recognition\")\n",
        "image_label = tk.Label(root)\n",
        "gesture_label = tk.Label(root, text=\"Gesture:\")\n",
        "start_button = tk"
      ],
      "metadata": {
        "id": "t80yfwfgo2b-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}